# AHA - again: it's nothing (really) new.

Just an orchestra with already living musicians - playing with existing instruments - a slightly more modernly fit scheme.

Things already pointing into the direction "needing to let go of folder/filenames":

  - browser bookmarks.
  - online stored "files": retrieved by query by tags (date, event, person, etc)
  - application/OS configuration: settings are not fixed-structured, but more commonly being searched by keyword(s).
    (eg AndroidOS, windows+linux+mac "start menu")
  - desktop search tools (databases and crawlers).

When I add a bookmark to my Firefox, I make simple use of 1-3 tags. I have quite some experience with how I find my stuff - and how I guess someone else may also be able to.

It feels quite natural, actually.

You just "save - tag - forget".
And search-query-find - and then "use" it at any later point.

Now many systems can "tag" in some form (and store in some form(s)) - yet, a generic way of putting "data into a relation with another data" doesn't really seem to exist.

At least not in ways that feel obvious or easy to - well, anyone.
Me as developer and metadata engineer, included.

Smoking heads.

However, imagine having URL-style addressing of fuzzy-colliding IDs of any-and-all data objects across your whole computing environment. Online as well as offline - and well kept even on stored away (ancient even) digital carriers.

As robust as plain-text, and as reliable as a mature filesystem.


You could even read it using a hex-editor searching for keywords on a harddrive dump - ignoring the filesystem actually. That would be awesome.

The only reasons I could imagine why binary is preferred over plaintext as working format for a filesystem:
Performance and storage.

I may fall into a chess-game trap.
Maybe it seems that this little more metadata overhead required for plaintext storage of key=values, and relationship/link information as I've anticipated. And the whole concept falls apart.

However, my assumptions regarding this being perfectly fine, are:

  * The very same data is already stored for many files:
    Every metadata entry stored as XML or other plaintext metadata is already using the same amount of bytes.
    Due to XML markup, possibly even slightly more than if converted to holodeck key=value/relationship layout.

  * Given the ratio between audiovisual media payload and even explicitly verbosely annotated metadata (including images): it can be assumed that "relevant" metadata memory requirements shall be considerably small.

  * If related objects become commonly accepted because they're behaving reliably and comfortably (enough):
    Any existing data-about-data already takes up the space it does (including backups and distributed copies).
  
  * Considering that the AHA design is drafting a storage network that supports privacy-aware "offline" - yet any Object Pools can easily be connected - and inter-referring to - other Objects - anywhere, as long as the protocols match.

  * So the main question may be more: how (and if?) to be able to quality-of-service balance/distribute the data according to maps of the interconnected "object spaces"?



