# Enter Focus Search

Do you know the game "[HyperRogue](https://en.wikipedia.org/wiki/HyperRogue)" on Linux?
It's awesome.
Quite trippy though: Because the visual plane - the world, so to speak - is laid out differently, than we're naturally "used to". Imagine you're in a room that's not square, but arranged as a truncated order-7 triangular tiling by default (with a few exceptions) - imagine a Pentagon with 7 sides - so when you enter another room in that world, it's bigger on the inside.

A bit like a [Tardis](https://en.wikipedia.org/wiki/TARDIS). Cool.

It feels totally natural after playing it for some time.
Now imagine traversing real data objects that you'd like to work and play with - like in a hyperbolic virtual "space" metaphor.
And the objects would be the "tiles" you walk onto/into.

Or merely using this visual metaphor to imagine the layout of the index-model-data that you interact with - which you query. Imagine, while you do your digital doings, since each and every single anything that makes up your (G)UI is a Data Object. So if I'm in my Object Browser, looking at old studio recordings - and I'd ask "who's the author?", the 'HAlodeck couuld use "what I'm looking at" to re-weight and order its responses.

  * How many recordings are there?
  * What is this? (Okay, this would be AI-stuff)
  * Can you show me more by the same creator or date-range?

Imagine you could "query" at any point, at any "Object". Anything you could image to "click" or refer to by an form of "unique-enough" identifier.

From any point.
Your interest is where you're focus is.

So the weights of the relevant results of your queries are being optimized towards "I'm asking from where I'm at - and by what I'm looking at - on my device".
Should be relatively easy?

I honestly don't know how Indexers work still. They're an amazing mystery too me.

I should read up on the docs, and get one running for my local protoype, though.
I want fulltext search to be quick and responsive.
The average "home-to-professional" users can't have more data to crunch than a average-larger museum collection?
And I don't think their webservers are running at full capacity.
I think they're mostly bored. So it should run fine on the average up-to-date-now hardware (or from year ~2010 on).
I'm talking quad-core 3-point-something GHz with hyper-cores or whatever - and 32 GB of DDR4, and fast NVMes for cheap. This will do. For quite a while.
Quote me on that. If it becomes sluggish, there's something off with something else. ;)


# Example:

> If i would like to do something to the network connection of a download-task running in a process in my currently focused terminal window?

If it were AHAlodeck system, by right-clicking that terminal window, the AHAlodeck OS would shift the weight/importance of any context (related Objects) - and adjust the indexing.

Therefore, if my preset profile enables to display "network" or "advanced" settings (=metadata, related objects, links) - I can simply traverse the Object-path from there.

Network brings me to the context of whatever network connections are - from focus "here" to "around" (like internet connection).

And since we came from the context of "that terminal window", any Object (=process) having a pointer/reference to it shows up - and is grouped (just like any other object) by its tag "network" or other.

You could literally get lost in the woods.
But why should you?

You can at all times "go back home" or simply quit and go out or reset.

And then you turn right - on your 2nd screen, where you have a web-browser with several tabs open. You can right-click literally *anything* you see - since any already related-object programming languages/designs - can be fully mapped to AHAlodeck storage/OS/computing environment.

And depending on "what" you right-click on: the search focus (weight/presets) will shift accordingly.
Speeding up your actual search, and making it easier to rather "navigate" (=re-sort/group/filter) the results. Then refine/search again.

# So, there's no "fixed search box" you have to go-to everytime you would like to navigate (by query) through a digital setup.

That feature to "query" - the "would you like to know more?" button.
And any Object is offering one.
Similar like to "toString()" method introduced already in the earlier days of Object Oriented (OO) programming. It was a big paradigm shift that boosted and changed the digital environment still very present in the status-quo today.

Now there are a few human-computer-interface (HCI) paradigms coming to their limits:

  * files-in-folders.
  * filenames.
  * foldernames.
  * folder-structure.
  * file/foldername syntax.
  * metadata schemata and standards.
  * having to pre-allocate digital space.
  * partitions as we know it.
  * meta+data asset management (full lifecycle+) state-of-the-art
  * embedded metadata.
  * excel tables for collection management.

How come everyone is not-too-happy with their "asset handling" situation in terms of long-term/safe storage, backup - and ease or price of use actually using that data and formats - and the ever-faster migration cycles.

And neverending metadata conversions and updates and imports/exports/mapping-code and tables.

It's pointing at the fact that the underlying `imaginary-binary` computing myths - well-serving, glorious UI-paradigms.

But their times are over.
The good news is: the new sexy boat to hop on is already waiting.

Converting almost any existing data, tools and even workflows - into a seamless Object-Storage-Data-Object Handling fun.


# Why to really consider a change of filesystem choice and usage?

When asking the question: "Hey, couldn't we just come up with a new, simple filesystem that just does it all?"

I wouldn't even dare to begin thinking about being able to contribute to filesystem-developer code, and it's serious and most fundamentally important piece of code.

But why am I bragging here then?
Because I believe that the `filesystem` to build the first AHAlodeck on already exists: MinIO + S3 protocol etc.

Going data-lake on collection data (first), combined with private collections.

Most information found on the web are fuzzed by sales-speak-buzzword-blah. Many articles available - and very informative - by companies working in that field of "big(ger) data" sound extremely "high-revenue class sales text with IT buzzwords".

So I find it hard to research on that topic.
And universities seem to be way too many years behind everything, lost in creating books and papers, referring to papers and books - all long outdated and barely useful in the real world anymore.



# Yes, they're already dead. But we can keep their memories.

Sorry if that sounds too cold.

I think I've really reached a point, after 2 years working on this prototype and data modelling, I'm a bit feeling like being in a relationship for too long with files-in-folders "FinF", and we're not getting anywhere anymore. And databases are completely on my blacklist. And doing the yet-another-one-time-mapping clever code hacks to import that one-or-maybe-3-collections into 1 common dataset.

Or maybe all into Europeana or EBUCore standards.
METS and PREMIS and MODS and ISAD-G and CSV all over the place.

I'm not willing to continue this any longer, while already knowing that there is an existing FOSS stack, waiting to be put together - in 8 months - to have a home-to-KMU-and-pro storage that will serve not only better than RAID, but already handles all embedded metadata tags in existence, for currently FOSS-licensed CLI-scriptable data formats.

I'm working with memory institutions.
And I honestly believe, that files-in-folders started itching since around 2005 and onwards.
How many application-specific local DB - and any other DataBase or file-format migrations have you encountered in your life? Sure, since late 2010s media and web file formats and protocols have greatly improved towards interoperability and openness.

But still putting still-too-much into the file-and-foldernames.
Everyone. Even me. All professionals - everyone.
And that's a big red pimple saying: "Actually, I should be a metadata field".

And after the n-th metadata (standard) conversion from A-to-B-then-later-whatever-C-pfff - arrrrgh!
And after the n-th of that: It's yet another pimple saying: "Actually, the filesystem should provide proper sections for storing nested key=value data. Why not?"

Also the speed of development and API/ABI/Layout changes all the time is insane, however this update-madness, combined with multi-device-and-realms-per-user as "the daily normal these days", also frictions against regular files-in-folders on a certain offline/home storage.

Having it on a floppy or harddrive on your desk is not "near enough" anymore.
Cloud.

What does "Cloud" do?
It stores your data as Objects on an Object Storage and extracts and handles and runs transactions on: meta+data.
Large scale.
For about 20 years productively for global free-for-all online usage.

Of course, their hardware stack is giganticomaticamaniac - yet: this simply *is* a proof-of-fact that Data Objects - so plain, pure data can be worked-, interacted-with in ways that transcend already from `FinF`.

Why still holding onto it?
Why ain't Object Storage the current (ex)FAT rolled out on any SDcard, USB-thumb-whatever-hard-and-soft storage drive - and devices support it as fluently as FAT16?


Seriously:
Compared to the Gazillions of Megamillions that are poured into hacking workarounds for large-scale global business and use-cases - this is nothing.
Implementing the basic environment for AHAlodeck and publishing it as an IETF standard for anyone to make use of.

Files-in-Folders are a thing of the past.
We've had great times!
Without FinF, there'd be no computing as we know and love it.
FinF, I'll miss you. Or maybe not.
I will definitely always remember you.
Even if I copy/paste all your existing metadata over to my Objects.
You'll be there.
You'll be part of their source and history.

Thank you.
And thanks to the clever heads and typing-fingers who made this a reality for all of us.
Why haven't I learned *your* names at school?
RMS Stallman, Tesla, and all the others we had to hear about on Youtube - not in schools or unis or professional documentaries.

So here's my good curse:
Whenever you're using "a computing device", simply entertain yourself in noticing how often whatever you're trying to do would just be *so damn much easier* if you could just type into a query-box what you are looking for.
On your *own* local setup.

Or simply click "save" on whatever. And it'll be just that: saved.
Stored on your storage (cloud-and-locally accessible), either auto- or manually-tagged at will and usecase.
And if you need more space, just add more space.

Without having to put all of your data up in someone's cloud first.
Or setup your own cloud first.
Or your own database-hells.
Whatever comes naturally or is available to you.


# Oh! Did I mention? This obsoletes a major part of digital data migration.

<Save!>

Where?
On the storage-pool assigned to it.
It being: This Object-context. A "Realm".

That's it.
Done.

You can tag Objects - so basically *anything* in a fully grown AHAlodeck - for/related do which and how many Realms you like. And anything else too. There is limit by standard to how wild you go in the metadata section of an object.

To be honest:
Maybe I'm talking about recursive key-length-value (KLV) data-formatting?
The engine in the background merely has a pre-coded indexer that works with this:

  * 1. key=value
  * 2. value can be any data: binary bit proof.
       and of *as much size as the whole pool can hold*.
  * 3. if interpreted as text, it's full ASCII and Unicode support.
  * 4. a key=value can refer to another key=value.
  * 5. therefore, one can tag another tag.
  * 6. there has to be a non-ever-colliding syntax to distinguish it from `literal` value.
       To distinguish between value-or-resolve-reference?

Therefore, it should be possible to store the "payload" as "payload=<DATA>" - so basically the payload becoming "yet another meta+data field".

How boring and convenient!

Seriously, the following tasks of data migration vanish on AHAlodeck:

  * any filename considerations.
  * any foldername considerations.
  * any file/folder structure handling. at all.
  * how many files-per-folder?
  * human-and-or-machine readable file/foldernames?
  * which database engine to commit for the next n years to?
  * which file-format to choose for your ... anything?
  * migrate/normalize/uncompress any complex or popular file format. Again.
  * which toolset and formats support your required/desired metadata tagging options?
  * I could go on.
  * And you probably could too, I guess?


# So, how does a data migration look on AHAlodeck?

On a properly administrated, local 'HAlodeck, you simply "save" the data Objects where you want to have them now.

By selecting "a data pool" you have the right access credentials to, and that has the properties you want your data to be stored on. This includes version control, process-logging, profiles and presets for almost anything.

You literally just connect 2 data collections of *any* Object Data storages - and simply copy whatever you like. Share and exchange as you see fit.

Any data Object looks like this now:

{
Type: Video Project
Title: Hello World.
Date: 2024-08-18
Creator: ...
Key: Value
Key: $VALUE2
VALUE2: Key
MD5: ...
HASH: $MD5
Related Objects: a, b, c, d, ...
Tags: ...
Folder: ...
Filename: ...
Filename: ...
}

Looks simple?
It really is. Think JSON meets XSLT on XSDs.

Yes, we're finished.
Once you've queried on the "one source" what you want to copy/ingest - then go to the target pool, hit "paste" and enter as many quick-tags - or upload/link extended metadata-or-related Objects - continue, and you're done.
You have live-access to any online data vocabulary/tag-norm collections.

And how to you import that data into your MAM or catalogue?
Simply by not doing anything:
If your catalogue or MAM is AHAlodeck-capable, or merely Object-Storage S3 protocol compatiblea, your new data can simply be "queried as usual".

Response-quality depending on the data from the source.
If you'd like to improve anything, that you'd have also done before *during* the import, you may do now.
And you can keep both: the old-and-new-metadata layout on the same Object, for as long as you like.


## Example: Backup of friend's music collection.

Attach Object - or FinF - storage pool, and select (by query or any interface you like) the files of that collection. copy-paste over to your desired target pool. For example: your local disk.
Done.
Your music application and all things music (that are Object-Storage designed) will simply now have "access to those songs, too".

And not only the songs:
Also anything else your friend had tagged along: artwork, scripts, websites, movies, documents, catalogue-entries, wikidata-wikipedia links - even 3D construction plans and model files. Including also Large Language Model files :)

Anything.
And no need to think about a foldername, or his-or-her filenames put into "your preferred order"...
Nothing. Done.

It doesn't matter if that collection was sorted-and-tagged professionally or otherwise - or not at all - or merely as-is copies from digital camera and recording equipment. Does not matter anymore.
FinF-information is extracted and copied onto their respective, standard(ized) metadata tags and that's it.

I believe by providing a plain KLV engine - self-referencing data allowed and encouraged,used and supported by design and default - may provide the most general means of describing everything and anything we can currently depict as a digital data object.

Or:
Simply have an existing Python parser in every regular filesystem implementation, and interpret a metadata-structure textblock as Object Class definition. Literally programming your very data directly.
Describing it in Python-code, which should be very straightforward to teach and learn - with fun! - and great satisfaaction, seeing how often this knowledge can now be applied to juggle digital data - rather than siffing through files-in-folders-in-warehouses-somewhere lost.

Yet, in the end it doesn't matter (except performance, I guess) how it is actually implemented.
The currently existing implementation in min.io should be more than sufficient to provide the desired functions of the 1st prototype AHA-PT1.


## Example 2: Merge 99 different collections from different sources into 1

Attach Object - or FinF - storage pool, and select (by query or any interface you like) the files of that collection. copy-paste over to your desired target pool. For example: your local disk.
Enter as much metadata as you see fit (see details above).
Click "Save".
Done.

Make sure there's enough space, and you have the right backup-and-failover-strategies in place for your main storage pools. You will need this anyways for *any other data* you're working with - and depending on - daily.

Yes, you can use sections of an existing pool. Ones that you have been given access to.
It doesn't matter if it's online or local: merely a different URI prefix to point to.

Your collections have been merged.
If you want to access that "data" - query its metadata and all - in your MAM, catalogue or plain Object Browser: Feel ree to do so.

The new data is "just there".
You can query it by any metadata you've added during copy/paste - and any other transformations or augmentations can be run now or at any later point in time at the same penalties and conditions.

You can relax now.
This is how easy it can be.

This is not yet another file-format.
This is another way of thinking digital data.

By applying an Object-Oriented thinking approach to any computing-meta+data use case.


## Graduating from FinF to final?

Depending on the existing data of your sources, this will gradually build up and improve upon, like this: 

Import once (during copy):

  * `creator = [P3ter B.]()`
  * `filename = hello_world`
  * `foldername = that/was/a/brilliant/syntax/idea/for_many_years/`
  * `foldername2 = D:\previous_disk\old_life\`
  * `filetype = image/jpeg`
  * `extension = jpg,jpeg`
  * `modification-date = YYYY-MM-DDT13:37:00`
  * `access-date = YYYY-MM-DDT13:37:00`
  * `creation-date = YYYY-MM-DDT13:37:00`
  * `key = value`
  * `key = value`
  * `...`
  * `width = value`
  * `height = value`
  * `colorspace = [RGB]()`
  * `access rights = ...`

You could read and understand that, right?
And if you had to teach that - it would be quite easy and fun, right?

Right? Right click! There you go.

Now we've lifted the existing filesystem properties we know - and easily had available to our Object Storage.
Easiest data-migration ever.
Yes. It was already a format migration.

So far, this importer has to be written still, but I expect it to be rather trivial in Python, considering existing programming libraries used to access all data sources and APIs required for extracting and copy-pasting it to the created target Object's meta-section.


## Continue walking on ice - but it holds.

**TODO: re-think or consider seriously having all meta=data, with `payload=<BINARY>`**

Assuming the engine is capable of doing "key=value" binary-safe, and allowing $key to refer to the value stored in "key=": We have a quick and simple, capable of even self-referencing its own tags by name/reference.

Seeing the above plaintext key=value right-click-default view of your files:
So how mysterious does the "file" look now?

If the payload encoding is understood/supported in your environment, you will probably get a meaningful preview - or icon shown, like now based on MIME TYPE and filename extension.

Of course you can `double-click` or equivalent it at any time - and it'll probably "just open somehow".

If your system does not yet understand the payload-encoding, the Object may have a simple web-link, pointing to different means of getting-and-setting-up one decoder to render and use that data.

For example: Every video clip has a link to VLC. And wikipedia about not only it's contents, but also its encoding data formats and protocols used. Why not?
That'd be something the video-tools would auto-embed. Like they do now with their library-versions, etc.
It's quite cool and useful actually. Thanks guys!

What would you like to do with that "Data" you have there now?
Want to put it in relationship to another "File"? Like connecting the cover-art (jpg) to an album (catalog-entry) to its tracks (mp3) - and copies in different formats for different reasons, like png, tiff, txt, csv, pdf, wav, flac, opus, m4a - aka "newpi" :)

> `Newpi`, because it's the new-PI! It's an infinite sequence (continuing as long as computing continues to be like now), yet unpredictable what the next "term" will be. Curious.


### Questioning the reasons for coming up with "yet another encoding of XY"?

I understand the differences between different data- and file-formats. I've worked all of this for over 20 years, personally as well as professionally. And i still kinda like it.

But will there ever be an end to "yet another new format"?

Why h265?
When will h266 be IT?
Will we have to keep supporting JPG and MP3 until the end of time?

And so on.

I am, by my professional gut-feeling, deeply questioning if it wouldn't be more preferable to *stay content* and happy with a given set of formats for a given set of use-cases.

Like a hammer, a sichel, a sense, a screwdriver, a piano - a guitar - instrument - anything:
There is a point, where a certain set of paradigms and possibilties shapes and morphs around a certain "final image" of a usefully-limited set of variations.

Until one comes up with "something completely different".
Which is in 99.9% of all cases: Just existing things put into a completely different context or use.

If we were condemned to be limited and stuck-with-for-all-eternity and on all computing devices ever to be or in the future: only this:

  * TXT (from ascii to unicode)
  * JPG, TIFF, PNG, GIF
  * WAV, FLAC, MP3, MP4
  * H.264/PCM
  * MOV, MKV, MXF
  * ODS, ODT, OpenOffice standard, PDF

I'd like to encourage a digital culture that is able to embrace being happy with well-supported-well-known-freely-available-for-anyone-for-over-20-years now.

So what's the reason for a new file format?
In "most cases": smaller files.
Or: new features.

The reason for "better looking" stopped IMO around yuv422p10le - and between 720p and FullHD.
And for audio &ge; 48kHz/32 float or 24 integer.
It's enough.
It's okay.
It's brilliant.
It's enough.

How often is a movie watched on a big leinwand?
And how often - in comparison is it watched on a home-screen?

And how often did it matter at all, whether or not "a good copy of a good movie" was shown "only between 720 and 1080"?
And how often would a good film maker make an even better movie, if "gear" was not the issue.
All films shot on either 720 or 1080. As you like man. All good.
Audio in PCM or Opus. I think we can afford that already.

What if we could even just be happy and agree to be happy with "mp3" or mp4 - and be good with it.
Leave it be. You can hear music in hyper-fidelity, wireless and encoded surround and whatnot.
IT's good.
It's okay.
Be happy.

Rather than worrying about keeping the WAVs and the original quality:
Make sure you keep the project and environments to be able to recreate it when needed. In the quality you desire and the format limitations provided.
You want to use it longer: use one of the above mentioned formats. It's a short list.
And you'll get to know it, as it stays like that for as long as it makes sense.

If your toolset/environment supports the above mentioned formats:
Keep it.
Like your favorite frying pan and pot and kitchen set.
Like a instruments in your life, you play in different times.
Do you really (be honest!) really, care if it was a good-resolution lossy but available copy - or rather *not* have it, but you knew that it would have been somewhat better?
Even if significantly better: So be it. Get on with it. Don't cry over rotten milk.
There are reasons why a great piece of (art) work was lost/destroyed/downsampled - overwritten, bit-broken - things happen.

Let it go.
And let's focus on the new exciting options, committing to a simpler stack would actually boost innovation!
Imagine how long any applications dealing with data would live without having to update all the time?
This would automagically de-boost the update-nonsense frenzy we're in right now.

Stay with the current "good enough" file formats of your computing "realms".


# Retro-support included by design

And sometimes you like to pull out that good-old-whatever - and be so happy to see it still working.
Or being able to put life back in, by repairing it.
Together with your kids and friends.

And enjoy that you've made something happen.
Again and again.

Having any data and application working with Objects like that, would very much almost natively support and encourage DIY-to-professional transformations of existing Objects and toolsets.

The FOSS environment and contributions in the field of retro-computing reverse-engineering and any-hardware-keepalive enthusiasts: brilliant! There we can see what stands the test of "computer" times - and why.

Simply "still having that old plug in and out" on all parts of your toolset you'd like to connect:
You're good. Done. Works. Most of the time.

Now in digital, there should be no need to "replace old transistors".
On the physical level of course, but not once things are functional digital data and programs.
Why should a program not support "JPG" anymore - from one day to another?
That regression would instantly impact so many users worldwide, you'd either get a fix, apology or very good reason for this happening.

In physical office and paper supplies - and hardware stores around the world we see it daily:
The parts that are around for a very long time either stay for (almost) good - or phase out for a (hopefully) good reason.

And still today, most machinery - especially professional and experimental, even prototype or special-use-case-military-hospital-grade tech: Nuts and bolts and Lena, A-Team plus MacGyver skills.

So why the Bluetooth-standard 3000+ and DDR89101112-duo, etc?
Lists of lists of lists of CPUs and protocols and features and pinouts and pins and powers and options and flashes and bricks - and reboots and resends, and compatibility issues, etc.

Why?

I believe many would be very happy with a working set of formats and features, at the computing power and constraints of around 2010 (take the average convenient PC or Mac at that time to get a range). 

We can do FullHD easily in that era already. PCM audio.
VLC on everything.
It was enough. It is good.
